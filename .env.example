# Gateway Configuration
# This is the only port exposed externally
PORT=8080
DOCS_DIR=./data/docs  # Local folder to mount into the container for ingestion

# LLM Provider Configuration
# Options: openai, ollama, anthropic, gemini
LLM_PROVIDER=ollama
LLM_MODEL=llama3.2:latest

# Embedding Configuration
EMBEDDING_PROVIDER=ollama
EMBEDDING_MODEL=nomic-embed-text

# Ollama Configuration (running on host machine)
OLLAMA_HOST=http://host.docker.internal:11434

# OpenAI Configuration (if using OpenAI)
# OPENAI_API_KEY=your_openai_api_key_here

# Anthropic Configuration (if using Anthropic)
# ANTHROPIC_API_KEY=your_anthropic_api_key_here

# Google Gemini Configuration (if using Gemini)
# GOOGLE_API_KEY=your_google_api_key_here

# Ingestion Configuration
INGEST_BATCH_SIZE=10
CHUNK_SIZE=512
CHUNK_OVERLAP=128

# Application Settings
DEBUG=false
LOG_LEVEL=INFO
